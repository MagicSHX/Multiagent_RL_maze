{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Import lib </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from tkinter import *\n",
    "from PIL import *\n",
    "from Model.Funtion_Bank import *\n",
    "from Model.DL_Network_Model import Net\n",
    "#from Model.Funtion_Bank import data_input\n",
    "\n",
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "#import random\n",
    "import time\n",
    "#from random import randint\n",
    "import itertools\n",
    "\n",
    "\n",
    "global btn_txt_matrix\n",
    "global btn_matrix\n",
    "global btn_refresh_matrix\n",
    "global Button\n",
    "global Current_play_ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Define Agent </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class agent_a:\n",
    "    def __init__(self, perception, state):\n",
    "        self.perception = perception\n",
    "        self.state = state\n",
    "\n",
    "        #print(self.verify_Matrix.argmax())\n",
    "    def restart(self):\n",
    "        print(self.input_string)\n",
    "        return('abc')\n",
    "    def next_action(self):\n",
    "        print('next action')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> define file_path and parameters </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'Data/training_data_input.csv'\n",
    "score_file_path = 'Data/training_data_score.csv'\n",
    "next_action_file_path = 'Data/training_data_next_action_taken.csv'\n",
    "\n",
    "file_name_model_latest_version = 'Model/model_latest_version.pt'\n",
    "file_name_model_last_version = 'Model/model_last_version.pt'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Game engine </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ground(row, column, wall_generated, target_generated, agent_a, agent_b):\n",
    "    rows = row * 3 + 1\n",
    "    columns = column * 3 + 1\n",
    "    init_ground = torch.tensor([[0 for x in range(columns)] for y in range(rows)], dtype=torch.float)\n",
    "    wall_generated.append([row, column, row, column * 2])\n",
    "    wall_generated.append([row, column, row * 2, column])\n",
    "    wall_generated.append([row * 2, column, row * 2, column * 2])\n",
    "    wall_generated.append([row, column * 2, row * 2, column * 2])\n",
    "    for line in wall_generated:\n",
    "        for i in range(line[0], line[2] + 1):\n",
    "            for j in range(line[1], line[3] + 1):\n",
    "                init_ground[i][j] = 1\n",
    "    for point in target_generated:\n",
    "        init_ground[point[0]][point[1]] = 2\n",
    "    for point in agent_a:\n",
    "        init_ground[point[0]][point[1]] = 3\n",
    "    for point in agent_b:\n",
    "        init_ground[point[0]][point[1]] = 4\n",
    "    return init_ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input(file_path):\n",
    "    #to explore: remove duplicatioin - if both input and score are same, then remove duplication\n",
    "    data_original = pd.read_csv(file_path, header = None)\n",
    "    #data_original = data_original.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    torch_tensor = torch.tensor(data_original.values, dtype=torch.float)\n",
    "    #print(torch_tensor)\n",
    "    return torch_tensor\n",
    "\n",
    "def DL_training_model():\n",
    "    indicator = 0\n",
    "    learning_rate = 0.1\n",
    "    epoch_size = 10000\n",
    "    steps_for_printing_out_loss = 1000\n",
    "    #cropped_perspective = np.array([4,5,6])\n",
    "    input_data = data_input(input_file_path)\n",
    "    score_data = data_input(score_file_path)\n",
    "    next_action_data = data_input(next_action_file_path)\n",
    "    \n",
    "    input = input_data\n",
    "    #print(input)\n",
    "    target = score_data\n",
    "    net = Net()\n",
    "    loss_functioin = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n",
    "    if indicator == 1:\n",
    "        state_dict_last_version = torch.load('Model/model_latest_version.pt')['state_dict']\n",
    "        net.load_state_dict(state_dict_last_version)\n",
    "    for i in range(1,epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        output[next_action_data == 0] = 0\n",
    "        #output[target == -2] = -2\n",
    "        #output[target == -1.1] = -1.1\n",
    "        loss = loss_functioin(output, target)\n",
    "        loss.backward()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "        # Does the update\n",
    "        optimizer.step()\n",
    "    \n",
    "    if indicator == 1:\n",
    "        copyfile(file_name_model_latest_version, file_name_model_last_version)\n",
    "    else:\n",
    "        torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_last_version)\n",
    "    torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_latest_version)\n",
    "\n",
    "\n",
    "def DL_model(cropped_perspective, model_version, state):\n",
    "    model_latest_version = Net()\n",
    "    state_dict_latest_version = torch.load(file_name_model_latest_version)['state_dict']\n",
    "    model_latest_version.load_state_dict(state_dict_latest_version)\n",
    "    next_vision = model_latest_version(torch.tensor(cropped_perspective.reshape(651), dtype=torch.float))\n",
    "    #next_vision[1] = 2\n",
    "    for i in range(0,4):\n",
    "        if state[i] == 1:\n",
    "            next_vision[i] = -3\n",
    "    #print(next_vision)\n",
    "\n",
    "    #print(next_vision)\n",
    "    \"\"\"\n",
    "    if ((state[0] == -1) and (state[1] == 1)) or ((state[0] == 0) and (state[1] == playground_w)):\n",
    "        next_vision[0] = -3\n",
    "    if ((state[0] == -1) and (state[3] == playground_w)) or ((state[0] == 0) and (state[3] == 1)):\n",
    "        next_vision[2] = -3\n",
    "    \"\"\"\n",
    "    #print('abc')\n",
    "    #print(next_vision)\n",
    "    next_step = next_vision.argmax()\n",
    "    action = next_step\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1. add a rule of agent movement:\\n- only 1 agent is allowed at the same location point\\n    - how to deal with confliction?\\n        - let all agents to predict the next step\\n        - list down confliction\\n        - how to decide, get an agreement?\\n            - scenario 1: all agents are within same group, and communication channel is available. And agreement can be made based on some rule set\\n                - what is the logic? \\n                    - maybe compare the Q value? \\n                    - or to figure out a way to calculate team performance impact on each scenario?\\n                    - or find a leader to decide?\\n                    - others?\\n            - scenario 2: not all agents are within same group, and communication channel is not available partially or completely\\n                - what is the solution?\\n                - \\n- wall\\n\\n\\n- each individual location point, to use an array to represent the current statement:\\n    - 0: None\\n    - 1: Wall\\n    - 2: target\\n    - 3: agent group A\\n    - 4: agent group B\\n    - TBC: how to indicate last moment of Turth?\\n        - maybe to use recurring NN\\n    - initial multi-agent location generation VS wall\\n    \\n\\n- target:\\n    - maybe assign different score to each target with some logic can be observed by agent? (enhancement)\\n    - to add a target \\n    - target can be moved as well(enhancement)\\n    - to play the agent A to chase against agent B (enhancement)\\n\\n- agent:\\n    - 0-8? or 0-5?\\n    - visioin control\\n        - only explored location is visible\\n        - communication is allowed within certain agent group\\n    - leader\\n    - small group of agent within agent group\\n    - \\n\\n- Score:\\n    - write a game engine:\\n        - option 1:\\n            - when to stop\\n                - within 200 steps, who gets more score, then win.\\n            - what is the score\\n                - the difference of target between two groups of agent?\\n    - DQ learning:\\n        - any other algorithm?\\n    \\n    - driven by comparation or his or her best peerformance? \\n        - Like in real world, kids used to be compared with others' performance may be better but there is a risk of aggressive\\n    \\n    - individual outstanding achivement\\n    - others has team reward\\n    \\n                \\n- agent group:\\n    - for now, it is 2 groups.\\n    - in future, we can try multi-groups together.\\n\\n\\n\\n- UI:\\n    - display result\\n\\n- result comparation:\\n    - using python to draw Dashboard and do comparation\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. add a rule of agent movement:\n",
    "- only 1 agent is allowed at the same location point\n",
    "    - how to deal with confliction?\n",
    "        - let all agents to predict the next step\n",
    "        - list down confliction\n",
    "        - how to decide, get an agreement?\n",
    "            - scenario 1: all agents are within same group, and communication channel is available. And agreement can be made based on some rule set\n",
    "                - what is the logic? \n",
    "                    - maybe compare the Q value? \n",
    "                    - or to figure out a way to calculate team performance impact on each scenario?\n",
    "                    - or find a leader to decide?\n",
    "                    - others?\n",
    "            - scenario 2: not all agents are within same group, and communication channel is not available partially or completely\n",
    "                - what is the solution?\n",
    "                - \n",
    "- wall\n",
    "\n",
    "\n",
    "- each individual location point, to use an array to represent the current statement:\n",
    "    - 0: None\n",
    "    - 1: Wall\n",
    "    - 2: target\n",
    "    - 3: agent group A\n",
    "    - 4: agent group B\n",
    "    - TBC: how to indicate last moment of Turth?\n",
    "        - maybe to use recurring NN\n",
    "    - initial multi-agent location generation VS wall\n",
    "    \n",
    "\n",
    "- target:\n",
    "    - maybe assign different score to each target with some logic can be observed by agent? (enhancement)\n",
    "    - to add a target \n",
    "    - target can be moved as well(enhancement)\n",
    "    - to play the agent A to chase against agent B (enhancement)\n",
    "\n",
    "- agent:\n",
    "    - 0-8? or 0-5?\n",
    "    - visioin control\n",
    "        - only explored location is visible\n",
    "        - communication is allowed within certain agent group\n",
    "    - leader\n",
    "    - small group of agent within agent group\n",
    "    - \n",
    "\n",
    "- Score:\n",
    "    - write a game engine:\n",
    "        - option 1:\n",
    "            - when to stop\n",
    "                - within 200 steps, who gets more score, then win.\n",
    "            - what is the score\n",
    "                - the difference of target between two groups of agent?\n",
    "    - DQ learning:\n",
    "        - any other algorithm?\n",
    "    \n",
    "    - driven by comparation or his or her best peerformance? \n",
    "        - Like in real world, kids used to be compared with others' performance may be better but there is a risk of aggressive\n",
    "    \n",
    "    - individual outstanding achivement\n",
    "    - others has team reward\n",
    "    \n",
    "    - reward shouldl be higher if game can be finished fast - this should be considered as part of DQ?\n",
    "                \n",
    "- agent group:\n",
    "    - for now, it is 2 groups.\n",
    "    - in future, we can try multi-groups together.\n",
    "\n",
    "- build a dostributed system to support multi-agent.\n",
    "\n",
    "- optimize the DL learning model:\n",
    "    - enhance model archtect\n",
    "    - tune the parameters\n",
    "\n",
    "- UI:\n",
    "    - display result\n",
    "\n",
    "- result comparation:\n",
    "    - using python to draw Dashboard and do comparation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DL_training_model()\n",
    "#DL_model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Main entrance </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 28]]\n",
      "[[15, 24]]\n",
      "[[18, 18]]\n",
      "[[18, 18]]\n",
      "[[17, 26]]\n",
      "[[17, 26]]\n",
      "[[18, 22]]\n",
      "[[18, 22]]\n",
      "agent_A_current_round_score: 4\n",
      "agent_B_current_round_score: 1\n",
      "agent_A_final_score: 3\n",
      "agent_B_final_score: -3\n",
      "total_current_round: 30\n"
     ]
    }
   ],
   "source": [
    "def step_movement(i):\n",
    "    switcher={\n",
    "    0:[-1, 0],\n",
    "    1:[0, -1],\n",
    "    2:[1, 0],\n",
    "    3:[0, 1],\n",
    "    4:[0, 0]\n",
    "    }\n",
    "    return switcher.get(i,\"Invalid next action\")\n",
    "\n",
    "def remove_duplication_in_lists(k):\n",
    "    if not k:\n",
    "        return []\n",
    "    elif len(k) == 1:\n",
    "        print(k)\n",
    "        return list(k)\n",
    "    else:\n",
    "        res = [] \n",
    "        for i in k: \n",
    "            if i not in res: \n",
    "                res.append(i)\n",
    "        return res\n",
    "\n",
    "    \n",
    "\n",
    "def agent_next_location(play_ground_numpy, agent_current_location, model_version, row, column):\n",
    "    #centre_point = [15, 23]\n",
    "    matrix_trancated = trancate_matrix(play_ground_numpy, agent_current_location, row, column)\n",
    "    #print(matrix_trancated)    #21, 31\n",
    "    state = []\n",
    "    for k in range(0, 4):\n",
    "        location = np.add(agent_current_location, step_movement(k))\n",
    "        state.append(play_ground_numpy[location[0], location[1]])\n",
    "    #print('state')\n",
    "    #print(state)\n",
    "        \n",
    "    next_action = DL_model(matrix_trancated,1,state)\n",
    "    next_action = np.flip(next_action.cpu().numpy())\n",
    "    #print(step_movement(next_action))\n",
    "    next_location = np.add(agent_current_location, step_movement(next_action))\n",
    "    return matrix_trancated.reshape(651), next_action, [next_location[0], next_location[1]]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DQ_ratio = 0.9\n",
    "    row = 10\n",
    "    column = 15\n",
    "    no_of_wall = 3\n",
    "    no_of_target = 7\n",
    "    no_of_agent = 5\n",
    "    \n",
    "    #initial position\n",
    "    wall_generated = wall_generation(row, column, no_of_wall)\n",
    "    target_generated = target_generation(row, column, no_of_target)\n",
    "    agent_a = agent_generation(row, column, no_of_agent)\n",
    "    agent_b = agent_generation(row, column, no_of_agent)\n",
    "    game_rounds = 30\n",
    "    agent_A_current_round_score = 0\n",
    "    agent_B_current_round_score = 0\n",
    "\n",
    "    agent_A_next_action = [[] for i in range(0, no_of_agent)]\n",
    "    agent_A_perceptioin = [[] for i in range(0, no_of_agent)]\n",
    "    agent_A_next_action_array = [[] for i in range(0, no_of_agent)]\n",
    "    agent_A_score = [[] for i in range(0, no_of_agent)]\n",
    "    agent_B_next_action = [[] for i in range(0, no_of_agent)]\n",
    "    agent_B_perceptioin = [[] for i in range(0, no_of_agent)]\n",
    "    agent_B_next_action_array = [[] for i in range(0, no_of_agent)]\n",
    "    agent_B_score = [[] for i in range(0, no_of_agent)]\n",
    "\n",
    "    \n",
    "    for current_round in range(0, game_rounds):\n",
    "        tensor_play_ground = play_ground(row, column, wall_generated, target_generated, agent_a, agent_b)\n",
    "        \n",
    "        tensor_play_ground_numpy = tensor_play_ground.clone()\n",
    "        play_ground_numpy = np.flip(tensor_play_ground_numpy.cpu().numpy(), 0)\n",
    "        \n",
    "        model_version = 1\n",
    "        \n",
    "        agent_A_found_target = []\n",
    "        agent_B_found_target = []\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(agent_a)\n",
    "        for agent_count in range(0, no_of_agent):\n",
    "            agent_a_perception, agent_a_next_action, agent_a[agent_count] = agent_next_location(play_ground_numpy, agent_a[agent_count], model_version, row, column)\n",
    "            agent_b_perception, agent_b_next_action, agent_b[agent_count] = agent_next_location(play_ground_numpy, agent_b[agent_count], model_version, row, column)\n",
    "\n",
    "            agent_A_perceptioin[agent_count].append(agent_a_perception)\n",
    "            agent_A_next_action[agent_count].append(agent_a_next_action)\n",
    "            action_array = [0] * (no_of_agent)\n",
    "            action_array[agent_a_next_action] = 1\n",
    "            agent_A_next_action_array[agent_count].append(action_array)\n",
    "            agent_A_score[agent_count].append([0] * (no_of_agent))\n",
    "            \n",
    "            agent_B_perceptioin[agent_count].append(agent_b_perception)\n",
    "            agent_B_next_action[agent_count].append(agent_b_next_action)\n",
    "            action_array = [0] * (no_of_agent)\n",
    "            action_array[agent_b_next_action] = 1\n",
    "            agent_B_next_action_array[agent_count].append(action_array)\n",
    "            agent_B_score[agent_count].append([0] * (no_of_agent))\n",
    "            \n",
    "            for item_array in target_generated:\n",
    "                #print(item_array)\n",
    "                if np.equal(agent_a[agent_count],item_array).all():\n",
    "                    agent_A_found_target.append(agent_a[agent_count])\n",
    "                    break\n",
    "            for item_array in target_generated:\n",
    "                if np.equal(agent_b[agent_count],item_array).all():\n",
    "                    agent_B_found_target.append(agent_b[agent_count])\n",
    "                    break\n",
    "        agent_A_found_target = remove_duplication_in_lists(agent_A_found_target)\n",
    "        agent_B_found_target = remove_duplication_in_lists(agent_B_found_target)\n",
    "        full_found_target = agent_A_found_target + agent_B_found_target\n",
    "        full_found_target = remove_duplication_in_lists(agent_A_found_target + agent_B_found_target)\n",
    "        #print(full_found_target)\n",
    "        for item in full_found_target:\n",
    "            target_generated.remove(item)\n",
    "        agent_A_current_round_score += len(agent_A_found_target)\n",
    "        agent_B_current_round_score += len(agent_B_found_target)\n",
    "        total_current_round = current_round + 1\n",
    "        if not target_generated:\n",
    "            break\n",
    "    \n",
    "    #print(agent_A_score)\n",
    "    #print(agent_B_score)\n",
    "\n",
    "    agent_A_final_score = agent_A_current_round_score - agent_B_current_round_score\n",
    "    agent_B_final_score = agent_A_final_score * -1\n",
    "    print(\"agent_A_current_round_score: \" + str(agent_A_current_round_score))\n",
    "    print(\"agent_B_current_round_score: \" + str(agent_B_current_round_score))\n",
    "    print(\"agent_A_final_score: \" + str(agent_A_final_score))\n",
    "    print(\"agent_B_final_score: \" + str(agent_B_final_score))\n",
    "    print(\"total_current_round: \" + str(total_current_round))\n",
    "    \n",
    "    for agent_count in range(0, no_of_agent):\n",
    "        for k in range(0, total_current_round):\n",
    "            agent_A_score[agent_count][-(k+1)][agent_A_next_action[agent_count][-(k+1)]] = ((DQ_ratio)**(k)) * agent_A_final_score\n",
    "            agent_B_score[agent_count][-(k+1)][agent_B_next_action[agent_count][-(k+1)]] = ((DQ_ratio)**(k)) * agent_B_final_score\n",
    "    \n",
    "    DL_model_input_data = []\n",
    "    DL_model_output_data = []\n",
    "    next_action_taken = []\n",
    "    next_action_taken_array = []\n",
    "    for agent_count in range(0, no_of_agent):\n",
    "        DL_model_input_data = DL_model_input_data + agent_A_perceptioin[agent_count]\n",
    "        DL_model_output_data = DL_model_output_data + agent_A_score[agent_count]\n",
    "        next_action_taken = next_action_taken + agent_A_next_action[agent_count]\n",
    "        next_action_taken_array = next_action_taken_array + agent_A_next_action_array[agent_count]\n",
    "    for agent_count in range(0, no_of_agent):\n",
    "        DL_model_input_data = DL_model_input_data + agent_B_perceptioin[agent_count]\n",
    "        DL_model_output_data = DL_model_output_data + agent_B_score[agent_count]\n",
    "        next_action_taken = next_action_taken + agent_B_next_action[agent_count]\n",
    "        next_action_taken_array = next_action_taken_array + agent_B_next_action_array[agent_count]\n",
    "\n",
    "    input_df = pd.DataFrame(DL_model_input_data)\n",
    "    input_df.to_csv(input_file_path, index=False, mode='a', header=False)\n",
    "    next_action_taken_df = pd.DataFrame(next_action_taken_array)\n",
    "    next_action_taken_df.to_csv(next_action_file_path, index=False, mode='a', header=False)\n",
    "    score_df = pd.DataFrame(DL_model_output_data)\n",
    "    score_df.to_csv(score_file_path, index=False, mode='a', header=False)\n",
    "    #print(DL_model_output_data)\n",
    "    \n",
    "    \n",
    "        #print(agent_a)\n",
    "    #pd.DataFrame(matrix_trancated.reshape(1, 21 * 31)).to_csv(input_file_path, index=False, mode='a', header=False)\n",
    "    #input_status_df.to_csv(input_file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (epoch: 1000): 0.13188852\n",
      "Loss (epoch: 2000): 0.09042186\n",
      "Loss (epoch: 3000): 0.07135681\n",
      "Loss (epoch: 4000): 0.06931289\n",
      "Loss (epoch: 5000): 0.06924354\n",
      "Loss (epoch: 6000): 0.0688283\n",
      "Loss (epoch: 7000): 0.06876992\n",
      "Loss (epoch: 8000): 0.06876029\n",
      "Loss (epoch: 9000): 0.06874372\n",
      "Loss (epoch: 10000): 0.06873153\n"
     ]
    }
   ],
   "source": [
    "DL_training_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[[15, 18], [13, 26], [13, 28]]\n",
      "[12, 17]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DL_model(matrix_trancated,1,1)\n",
    "print(agent_A_found_target)\n",
    "#DL_model(matrix_trancated,1,1)\n",
    "print(agent_B_found_target)\n",
    "print(target_generated)\n",
    "\n",
    "print(item)\n",
    "print(full_found_target)\n",
    "agent_B_found_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def remove_duplication_in_lists(k):\n",
    "    if not k:\n",
    "        return []\n",
    "    elif len(k) == 1:\n",
    "        print(k)\n",
    "        return list(k)\n",
    "    else:\n",
    "        res = [] \n",
    "        for i in k: \n",
    "            if i not in res: \n",
    "                res.append(i)\n",
    "        return res\n",
    "\n",
    "k = agent_A_found_target + agent_B_found_target + agent_B_found_target\n",
    "print(k)\n",
    "k = remove_duplication_in_lists(k)\n",
    "print(k)\n",
    "#return list(k for k,_ in itertools.groupby(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 16], [17, 18], [19, 16], [19, 16], [19, 16]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d7d29cfbc717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_B_found_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_A_found_target\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0magent_B_found_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplay_ground_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "print(agent_a)\n",
    "print(i)\n",
    "print(agent_B_found_target)\n",
    "print(agent_A_found_target + agent_B_found_target)\n",
    "print(play_ground_numpy)\n",
    "#print(matrix_trancated)\n",
    "np.savetxt(\"foo.csv\", play_ground_numpy, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = remove_duplication_in_lists([[12,3]])\n",
    "print('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_B_score = [[] for i in range(0, 6)]\n",
    "print(agent_B_score)\n",
    "agent_A_found_target\n",
    "len(agent_A_found_target)\n",
    "\n",
    "\n",
    "agent_A_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leader\n",
    "#predict other agent next action\n",
    "#against other agent's prediction\n",
    "#communicatoin matrix\n",
    "#memory\n",
    "#remove some wall\n",
    "#set up starting point for agent a & b\n",
    "#speed & count parameter\n",
    "#use an array to present each point - []\n",
    "[0,1,0,0,0,0,0,0,0,0,0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
