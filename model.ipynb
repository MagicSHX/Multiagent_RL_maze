{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Import lib </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "from tkinter import *\n",
    "from PIL import *\n",
    "from Model.Funtion_Bank import *\n",
    "from Model.DL_Network_Model import Net\n",
    "#from Model.Funtion_Bank import data_input\n",
    "\n",
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "#import random\n",
    "import time\n",
    "#from random import randint\n",
    "\n",
    "\n",
    "global btn_txt_matrix\n",
    "global btn_matrix\n",
    "global btn_refresh_matrix\n",
    "global Button\n",
    "global Current_play_ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Define Agent </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class agent_a:\n",
    "    def __init__(self, perception, state):\n",
    "        self.perception = perception\n",
    "        self.state = state\n",
    "\n",
    "        #print(self.verify_Matrix.argmax())\n",
    "    def restart(self):\n",
    "        print(self.input_string)\n",
    "        return('abc')\n",
    "    def next_action(self):\n",
    "        print('next action')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> define file_path and parameters </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'Data/training_data_input.csv'\n",
    "score_file_path = 'Data/training_data_score.csv'\n",
    "next_action_file_path = 'Data/training_data_next_action_taken.csv'\n",
    "\n",
    "file_name_model_latest_version = 'Model/model_latest_version.pt'\n",
    "file_name_model_last_version = 'Model/model_last_version.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Game engine </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ground(row, column, wall_generated, target_generated, agent_a, agent_b):\n",
    "    rows = row * 3\n",
    "    columns = column * 3\n",
    "    init_ground = torch.tensor([[0 for x in range(columns)] for y in range(rows)], dtype=torch.float)\n",
    "    wall_generated.append([row, column, row, column * 2])\n",
    "    wall_generated.append([row, column, row * 2, column])\n",
    "    wall_generated.append([row * 2, column, row * 2, column * 2])\n",
    "    wall_generated.append([row, column * 2, row * 2, column * 2])\n",
    "    for line in wall_generated:\n",
    "        for i in range(line[0], line[2] + 1):\n",
    "            for j in range(line[1], line[3] + 1):\n",
    "                init_ground[i][j] = 1\n",
    "    for point in target_generated:\n",
    "        init_ground[point[0]][point[1]] = 2\n",
    "    for point in agent_a:\n",
    "        init_ground[point[0]][point[1]] = 3\n",
    "    for point in agent_b:\n",
    "        init_ground[point[0]][point[1]] = 4\n",
    "    return init_ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input(file_path):\n",
    "    #to explore: remove duplicatioin - if both input and score are same, then remove duplication\n",
    "    data_original = pd.read_csv(file_path, header = None)\n",
    "    #data_original = data_original.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    torch_tensor = torch.tensor(data_original.values, dtype=torch.float)\n",
    "    #print(torch_tensor)\n",
    "    return torch_tensor\n",
    "\n",
    "def DL_training_model():\n",
    "    indicator = 0\n",
    "    learning_rate = 0.1\n",
    "    epoch_size = 100\n",
    "    steps_for_printing_out_loss = 10\n",
    "    #cropped_perspective = np.array([4,5,6])\n",
    "    input_data = data_input(input_file_path)\n",
    "    score_data = data_input(score_file_path)\n",
    "    input = input_data\n",
    "    #print(input)\n",
    "    target = score_data\n",
    "    net = Net()\n",
    "    loss_functioin = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n",
    "    if indicator == 1:\n",
    "        state_dict_last_version = torch.load('Model/model_latest_version.pt')['state_dict']\n",
    "        net.load_state_dict(state_dict_last_version)\n",
    "    for i in range(1,epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(input)\n",
    "        output[target == -2] = -2\n",
    "        #output[target == -1.1] = -1.1\n",
    "        loss = loss_functioin(output, target)\n",
    "        loss.backward()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "        # Does the update\n",
    "        optimizer.step()\n",
    "    \n",
    "    if indicator == 1:\n",
    "        copyfile(file_name_model_latest_version, file_name_model_last_version)\n",
    "    else:\n",
    "        torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_last_version)\n",
    "    torch.save({'state_dict': net.state_dict(),'optimizer': optimizer.state_dict()}, file_name_model_latest_version)\n",
    "\n",
    "\n",
    "def DL_model(cropped_perspective, model_version, state):\n",
    "    model_latest_version = Net()\n",
    "    state_dict_latest_version = torch.load(file_name_model_latest_version)['state_dict']\n",
    "    model_latest_version.load_state_dict(state_dict_latest_version)\n",
    "    next_vision = model_latest_version(torch.tensor(cropped_perspective.reshape(651), dtype=torch.float))\n",
    "    for i in range(0,4):\n",
    "        if state[i] == 1:\n",
    "            next_vision[i] = -3\n",
    "    #print(next_vision)\n",
    "    \"\"\"\n",
    "    if ((state[0] == -1) and (state[1] == 1)) or ((state[0] == 0) and (state[1] == playground_w)):\n",
    "        next_vision[0] = -3\n",
    "    if ((state[0] == -1) and (state[3] == playground_w)) or ((state[0] == 0) and (state[3] == 1)):\n",
    "        next_vision[2] = -3\n",
    "    \"\"\"\n",
    "    #print('abc')\n",
    "    #print(next_vision)\n",
    "    next_step = next_vision.argmax()\n",
    "    action = next_step\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. add a rule of agent movement:\\n- only 1 agent is allowed at the same location point\\n    - how to deal with confliction?\\n        - let all agents to predict the next step\\n        - list down confliction\\n        - how to decide, get an agreement?\\n            - scenario 1: all agents are within same group, and communication channel is available. And agreement can be made based on some rule set\\n                - what is the logic? \\n                    - maybe compare the Q value? \\n                    - or to figure out a way to calculate team performance impact on each scenario?\\n                    - or find a leader to decide?\\n                    - others?\\n            - scenario 2: not all agents are within same group, and communication channel is not available partially or completely\\n                - what is the solution?\\n                - \\n- wall\\n\\n\\n- each individual location point, to use an array to represent the current statement:\\n    - 0: None\\n    - 1: Wall\\n    - 2: target\\n    - 3: agent group A\\n    - 4: agent group B\\n    - TBC: how to indicate last moment of Turth?\\n        - maybe to use recurring NN\\n    - initial multi-agent location generation VS wall\\n    \\n\\n- target:\\n    - maybe assign different score to each target with some logic can be observed by agent? (enhancement)\\n    - to add a target \\n    - target can be moved as well(enhancement)\\n    - to play the agent A to chase against agent B (enhancement)\\n\\n- agent:\\n    - 0-8? or 0-5?\\n    - visioin control\\n        - only explored location is visible\\n        - communication is allowed within certain agent group\\n    - leader\\n    - small group of agent within agent group\\n    - \\n\\n- Score:\\n    - write a game engine:\\n        - option 1:\\n            - when to stop\\n                - within 200 steps, who gets more score, then win.\\n            - what is the score\\n                - the difference of target between two groups of agent?\\n    - DQ learning:\\n        - any other algorithm?\\n        \\n    \\n                \\n- agent group:\\n    - for now, it is 2 groups.\\n    - in future, we can try multi-groups together.\\n\\n\\n\\n- UI:\\n    - display result\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. add a rule of agent movement:\n",
    "- only 1 agent is allowed at the same location point\n",
    "    - how to deal with confliction?\n",
    "        - let all agents to predict the next step\n",
    "        - list down confliction\n",
    "        - how to decide, get an agreement?\n",
    "            - scenario 1: all agents are within same group, and communication channel is available. And agreement can be made based on some rule set\n",
    "                - what is the logic? \n",
    "                    - maybe compare the Q value? \n",
    "                    - or to figure out a way to calculate team performance impact on each scenario?\n",
    "                    - or find a leader to decide?\n",
    "                    - others?\n",
    "            - scenario 2: not all agents are within same group, and communication channel is not available partially or completely\n",
    "                - what is the solution?\n",
    "                - \n",
    "- wall\n",
    "\n",
    "\n",
    "- each individual location point, to use an array to represent the current statement:\n",
    "    - 0: None\n",
    "    - 1: Wall\n",
    "    - 2: target\n",
    "    - 3: agent group A\n",
    "    - 4: agent group B\n",
    "    - TBC: how to indicate last moment of Turth?\n",
    "        - maybe to use recurring NN\n",
    "    - initial multi-agent location generation VS wall\n",
    "    \n",
    "\n",
    "- target:\n",
    "    - maybe assign different score to each target with some logic can be observed by agent? (enhancement)\n",
    "    - to add a target \n",
    "    - target can be moved as well(enhancement)\n",
    "    - to play the agent A to chase against agent B (enhancement)\n",
    "\n",
    "- agent:\n",
    "    - 0-8? or 0-5?\n",
    "    - visioin control\n",
    "        - only explored location is visible\n",
    "        - communication is allowed within certain agent group\n",
    "    - leader\n",
    "    - small group of agent within agent group\n",
    "    - \n",
    "\n",
    "- Score:\n",
    "    - write a game engine:\n",
    "        - option 1:\n",
    "            - when to stop\n",
    "                - within 200 steps, who gets more score, then win.\n",
    "            - what is the score\n",
    "                - the difference of target between two groups of agent?\n",
    "    - DQ learning:\n",
    "        - any other algorithm?\n",
    "        \n",
    "    \n",
    "                \n",
    "- agent group:\n",
    "    - for now, it is 2 groups.\n",
    "    - in future, we can try multi-groups together.\n",
    "\n",
    "\n",
    "\n",
    "- UI:\n",
    "    - display result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DL_training_model()\n",
    "#DL_model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:black'> Main entrance </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n",
      "[15, 17]\n",
      "[11, 19]\n",
      "[14, 29]\n",
      "[19, 16]\n",
      "[12, 18]\n",
      "[14, 21]\n",
      "[11, 25]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def step_movement(i):\n",
    "    switcher={\n",
    "    0:[-1, 0],\n",
    "    1:[0, -1],\n",
    "    2:[1, 0],\n",
    "    3:[0, 1],\n",
    "    4:[0, 0]\n",
    "    }\n",
    "    return switcher.get(i,\"Invalid next action\")\n",
    "\n",
    "\n",
    "\n",
    "def agent_next_location(play_ground_numpy, agent_current_location, model_version, row, column):\n",
    "    #centre_point = [15, 23]\n",
    "    matrix_trancated = trancate_matrix(play_ground_numpy, agent_current_location, row, column)\n",
    "    #print(matrix_trancated)    #21, 31\n",
    "    state = []\n",
    "    for k in range(0, 4):\n",
    "        location = np.add(agent_current_location, step_movement(k))\n",
    "        state.append(play_ground_numpy[location[0], location[1]])\n",
    "    #print(state)\n",
    "        \n",
    "    next_action = DL_model(matrix_trancated,1,state)\n",
    "    next_action = np.flip(next_action.cpu().numpy())\n",
    "    #print(step_movement(next_action))\n",
    "    return (np.add(agent_current_location, step_movement(next_action)))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    row = 10\n",
    "    column = 15\n",
    "    no_of_wall = 10\n",
    "    no_of_target = 7\n",
    "    no_of_agent = 5\n",
    "    #initial position\n",
    "    wall_generated = wall_generation(row, column, no_of_wall)\n",
    "    target_generated = target_generation(row, column, no_of_target)\n",
    "    agent_a = agent_generation(row, column, no_of_agent)\n",
    "    agent_b = agent_generation(row, column, no_of_agent)\n",
    "    game_rounds = 20\n",
    "    agent_A_score = 0\n",
    "    agent_B_score = 0\n",
    "    \n",
    "    for current_round in range(0, game_rounds):\n",
    "        tensor_play_ground = play_ground(row, column, wall_generated, target_generated, agent_a, agent_b)\n",
    "        tensor_play_ground_numpy = tensor_play_ground.clone()\n",
    "        play_ground_numpy = np.flip(tensor_play_ground_numpy.cpu().numpy(), 0)\n",
    "        \n",
    "        model_version = 1\n",
    "        \n",
    "        agent_A_found_target = []\n",
    "        agent_B_found_target = []\n",
    "        \n",
    "        #print(agent_a)\n",
    "        for i in range(0, no_of_agent):\n",
    "            agent_a[i] = agent_next_location(play_ground_numpy, agent_a[i], model_version, row, column)\n",
    "            agent_b[i] = agent_next_location(play_ground_numpy, agent_b[i], model_version, row, column)\n",
    "        for item_array in target_generated:\n",
    "            print(item_array)\n",
    "            if np.equal(agent_a[i],item_array).all():\n",
    "                agent_A_found_target.append(agent_a[i])\n",
    "                break\n",
    "        for item_array in target_generated:\n",
    "            if np.equal(agent_b[i],item_array).all():\n",
    "                agent_B_found_target.append(agent_b[i])\n",
    "                break\n",
    "        agent_A_found_target = list(set(agent_A_found_target))\n",
    "        agent_B_found_target = list(set(agent_B_found_target))\n",
    "        print(agent_A_found_target)\n",
    "        full_found_target = list(set(np.add(agent_A_found_target, agent_B_found_target)))\n",
    "        print(full_found_target)\n",
    "        for item in full_found_target:\n",
    "            target_generated.remove(item)\n",
    "        agent_A_score += len(agent_A_found_target)\n",
    "        agent_B_score += len(agent_B_found_target)\n",
    "\n",
    "        \n",
    "        #print(agent_a)\n",
    "    #pd.DataFrame(matrix_trancated.reshape(1, 21 * 31)).to_csv(input_file_path, index=False, mode='a', header=False)\n",
    "    #input_status_df.to_csv(input_file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_model(matrix_trancated,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leader\n",
    "#predict other agent next action\n",
    "#against other agent's prediction\n",
    "#communicatoin matrix\n",
    "#memory\n",
    "#remove some wall\n",
    "#set up starting point for agent a & b\n",
    "#speed & count parameter\n",
    "#use an array to present each point - []\n",
    "[0,1,0,0,0,0,0,0,0,0,0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
